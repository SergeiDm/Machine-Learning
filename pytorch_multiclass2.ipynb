{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=5, n_informative=5, n_redundant=0, n_classes=4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "X_test_t = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "y_test_t = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=24, bias=True)\n",
      "  (fc4): Linear(in_features=24, out_features=4, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 32)\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=1.0)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 24)\n",
    "        self.fc4 = nn.Linear(24, 4)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Classifier()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.4345,  0.2599, -0.4428,  0.5273, -0.4238],\n",
      "        [-1.1420,  1.5063, -0.1439,  0.4210,  0.7373],\n",
      "        [ 2.6301,  0.0297, -0.2762, -0.1317,  0.4022],\n",
      "        [-0.6813, -0.4834, -0.1795,  0.0861, -0.3332],\n",
      "        [-0.0086,  0.5530, -0.3460, -0.6616, -0.6208],\n",
      "        [ 0.2777, -0.1962,  0.5358, -1.0017,  0.4319],\n",
      "        [ 1.3984,  0.7106,  1.0105, -0.3274, -0.1993],\n",
      "        [ 0.9489,  0.8337, -0.0905,  0.0811,  1.2139],\n",
      "        [-1.4157, -2.0593,  0.5694,  0.3177,  1.5895],\n",
      "        [ 2.3232, -0.4252, -0.6683,  0.7729, -2.5709],\n",
      "        [-1.0085, -1.8043,  1.1724, -0.7644, -2.0162],\n",
      "        [-1.4614, -0.5808,  2.4717,  0.6765, -0.7668],\n",
      "        [-0.0468, -0.4617, -1.4929,  0.9394,  0.0715],\n",
      "        [-0.6144, -0.0286, -1.7006,  0.1303, -0.9175],\n",
      "        [-0.0709, -0.2456, -1.4111, -1.5462,  0.5141],\n",
      "        [ 0.5092, -0.5950,  0.0190, -0.5872,  0.5012],\n",
      "        [ 0.1355, -0.1903, -0.5052, -2.3160, -0.2656],\n",
      "        [-0.9068, -1.2798,  0.1967,  0.8825,  1.2227],\n",
      "        [ 1.2506, -1.9323,  0.6031,  2.0230,  2.2108],\n",
      "        [-1.0527,  0.6647,  0.7387, -1.1740, -1.4380],\n",
      "        [ 0.0551, -0.7837,  0.0719,  0.1334, -1.2988],\n",
      "        [ 0.2077, -1.3276, -1.3373,  0.7136, -0.1606],\n",
      "        [ 1.0353, -1.8319,  0.9943, -0.6235,  0.3348],\n",
      "        [ 1.4279, -0.9078, -0.8116,  1.3305,  0.6102],\n",
      "        [-0.4531, -1.7041, -1.4366, -1.8512,  0.5896],\n",
      "        [ 0.1253, -1.2379,  1.5977, -0.6073, -0.3782],\n",
      "        [-1.1024,  0.2599, -0.8520,  0.3250, -2.6075],\n",
      "        [-0.0706,  0.2023, -1.9293,  0.6901,  0.5900],\n",
      "        [-0.6905, -1.0243,  0.7282,  0.2447,  0.0267],\n",
      "        [ 0.0675, -1.1878, -1.0146, -1.7212, -1.2383],\n",
      "        [ 0.1552, -0.6317,  0.0041,  0.2985, -0.9305],\n",
      "        [ 0.0226,  0.7321, -0.3285, -1.6313,  1.5462]], requires_grad=True)\n",
      "fc1.bias\n",
      "Parameter containing:\n",
      "tensor([-0.1935, -0.4259, -0.2921, -0.0563, -0.0883, -0.0823, -0.4003,  0.3876,\n",
      "        -0.4168,  0.1189,  0.2131,  0.1044, -0.0596,  0.3197, -0.0686, -0.3562,\n",
      "        -0.2409,  0.2313,  0.2097, -0.3189,  0.3345, -0.3677,  0.2010, -0.2831,\n",
      "        -0.1621, -0.0733, -0.2685, -0.1474,  0.4195, -0.4126, -0.4224, -0.0975],\n",
      "       requires_grad=True)\n",
      "fc2.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1561, -0.0813,  0.1665,  ...,  0.1160, -0.0591,  0.0186],\n",
      "        [ 0.1293,  0.1742, -0.0867,  ...,  0.1600, -0.1145,  0.0847],\n",
      "        [ 0.1572, -0.0895,  0.1105,  ..., -0.0685, -0.1614,  0.1217],\n",
      "        ...,\n",
      "        [-0.0080,  0.1574, -0.1289,  ...,  0.1524, -0.1120,  0.0088],\n",
      "        [-0.1526, -0.0647, -0.1499,  ...,  0.0339,  0.0059,  0.1617],\n",
      "        [-0.0105,  0.1351, -0.0646,  ..., -0.1550, -0.0728, -0.1346]],\n",
      "       requires_grad=True)\n",
      "fc2.bias\n",
      "Parameter containing:\n",
      "tensor([-0.1232, -0.0990, -0.0300,  0.0551, -0.1746,  0.0573, -0.0766,  0.1624,\n",
      "        -0.0757,  0.0596, -0.0879,  0.0077,  0.0597,  0.0630,  0.1289,  0.1399,\n",
      "         0.0107, -0.0409,  0.0663,  0.0558,  0.1724,  0.0879, -0.0021,  0.1260,\n",
      "        -0.0615, -0.1756, -0.0834, -0.0099,  0.1233,  0.1676, -0.0627, -0.0519,\n",
      "        -0.0317,  0.0749, -0.1622,  0.1373,  0.1460,  0.0169, -0.1219,  0.1074,\n",
      "         0.1271, -0.1331, -0.0212, -0.0933, -0.0549,  0.0279,  0.1277, -0.0632,\n",
      "        -0.1080, -0.0826, -0.1654, -0.0267, -0.1696, -0.1171,  0.0201, -0.0672,\n",
      "        -0.0401,  0.0156, -0.1654, -0.0235, -0.0343,  0.0431, -0.0360, -0.1076],\n",
      "       requires_grad=True)\n",
      "fc3.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0237,  0.0752, -0.0466,  ...,  0.0951, -0.0251, -0.0099],\n",
      "        [-0.0017, -0.0663,  0.0670,  ...,  0.0657, -0.0246,  0.1242],\n",
      "        [-0.0731,  0.1233, -0.0117,  ..., -0.0656, -0.0112,  0.0077],\n",
      "        ...,\n",
      "        [ 0.0148, -0.0025,  0.0741,  ..., -0.0864, -0.0431,  0.1235],\n",
      "        [-0.0448,  0.0498, -0.0934,  ...,  0.0629,  0.0052,  0.0497],\n",
      "        [-0.0851, -0.0251,  0.0841,  ..., -0.0695,  0.0446,  0.0234]],\n",
      "       requires_grad=True)\n",
      "fc3.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0035,  0.0485, -0.0556, -0.0108, -0.0693,  0.1014, -0.0824,  0.0567,\n",
      "         0.0330, -0.0761, -0.0986,  0.1121, -0.0454, -0.0282,  0.0255, -0.0462,\n",
      "        -0.0815,  0.0219, -0.0806, -0.0787, -0.0127, -0.0849, -0.1135,  0.0113],\n",
      "       requires_grad=True)\n",
      "fc4.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1909, -0.0980, -0.0389, -0.0554, -0.1065,  0.0610, -0.0398,  0.0847,\n",
      "          0.0183,  0.0167,  0.1354, -0.1895, -0.0833, -0.1206,  0.0732,  0.1942,\n",
      "         -0.1974,  0.0413, -0.1880, -0.0289,  0.1322, -0.1368, -0.0645,  0.1418],\n",
      "        [ 0.0548, -0.1450, -0.0191, -0.1710, -0.1897, -0.1756, -0.0634, -0.0004,\n",
      "         -0.1748, -0.0957, -0.0831, -0.1573,  0.1315, -0.1374,  0.1326, -0.1211,\n",
      "          0.0030, -0.1555, -0.0360,  0.0677,  0.0200,  0.0752, -0.0661, -0.1645],\n",
      "        [ 0.0199, -0.1654,  0.1565, -0.1928,  0.0245,  0.1581,  0.0091, -0.1376,\n",
      "         -0.1849,  0.1337,  0.1982,  0.0131, -0.0920, -0.0278, -0.2007,  0.0498,\n",
      "         -0.0078, -0.0054,  0.1191,  0.0585,  0.0598, -0.0059, -0.1754,  0.0769],\n",
      "        [-0.1490, -0.1976,  0.0762, -0.0330, -0.2033,  0.0441,  0.1732, -0.1293,\n",
      "          0.1305,  0.1585, -0.0857,  0.1799,  0.1970,  0.1671,  0.1401,  0.1507,\n",
      "          0.1934,  0.0324, -0.1190, -0.0426,  0.0225,  0.2015,  0.0998,  0.1804]],\n",
      "       requires_grad=True)\n",
      "fc4.bias\n",
      "Parameter containing:\n",
      "tensor([-0.1219, -0.1881, -0.0747,  0.0839], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(name)\n",
    "    print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss: 1.410114,             Validation loss: 1.275274\n",
      "Epoch 0, Training accuracy: 0.257500,             Validation accuracy: 0.510000\n",
      "Save the model --> model.pt\n",
      "Epoch 1, Training loss: 1.218260,             Validation loss: 1.098020\n",
      "Epoch 1, Training accuracy: 0.542500,             Validation accuracy: 0.607500\n",
      "Save the model --> model.pt\n",
      "Epoch 2, Training loss: 1.079079,             Validation loss: 0.953942\n",
      "Epoch 2, Training accuracy: 0.595000,             Validation accuracy: 0.660000\n",
      "Save the model --> model.pt\n",
      "Epoch 3, Training loss: 0.960562,             Validation loss: 0.874992\n",
      "Epoch 3, Training accuracy: 0.665000,             Validation accuracy: 0.682500\n",
      "Save the model --> model.pt\n",
      "Epoch 4, Training loss: 0.933285,             Validation loss: 0.827884\n",
      "Epoch 4, Training accuracy: 0.625000,             Validation accuracy: 0.705000\n",
      "Save the model --> model.pt\n",
      "Epoch 5, Training loss: 0.880729,             Validation loss: 0.794364\n",
      "Epoch 5, Training accuracy: 0.672500,             Validation accuracy: 0.712500\n",
      "Save the model --> model.pt\n",
      "Epoch 6, Training loss: 0.844055,             Validation loss: 0.762132\n",
      "Epoch 6, Training accuracy: 0.695000,             Validation accuracy: 0.715000\n",
      "Save the model --> model.pt\n",
      "Epoch 7, Training loss: 0.822196,             Validation loss: 0.734934\n",
      "Epoch 7, Training accuracy: 0.725000,             Validation accuracy: 0.737500\n",
      "Save the model --> model.pt\n",
      "Epoch 8, Training loss: 0.803245,             Validation loss: 0.714491\n",
      "Epoch 8, Training accuracy: 0.707500,             Validation accuracy: 0.735000\n",
      "Save the model --> model.pt\n",
      "Epoch 9, Training loss: 0.754452,             Validation loss: 0.693418\n",
      "Epoch 9, Training accuracy: 0.725000,             Validation accuracy: 0.755000\n",
      "Save the model --> model.pt\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "min_loss = np.Inf\n",
    "eps = 1e-2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    trainnig_accuracy = 0\n",
    "    for samples, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(samples)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item() * len(samples)\n",
    "        # Calculate accuracy\n",
    "        y_hat = torch.argmax(output, dim=1)\n",
    "        trainnig_accuracy += torch.sum(y_hat == labels).item()\n",
    "    else:\n",
    "        training_loss = training_loss / len(train_ds)\n",
    "        trainnig_accuracy = trainnig_accuracy / len(train_ds)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_loss = 0\n",
    "        validation_accuracy = 0\n",
    "        for samples, labels in test_loader:\n",
    "            output = model.forward(samples)\n",
    "            loss = criterion(output, labels)\n",
    "            validation_loss += loss.item() * len(samples)\n",
    "            # Calculate accuracy\n",
    "            y_hat = torch.argmax(output, dim=1)\n",
    "            validation_accuracy += torch.sum(y_hat == labels).item()\n",
    "        else:\n",
    "            validation_loss = validation_loss / len(test_ds)\n",
    "            validation_accuracy = validation_accuracy / len(test_ds)\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Training loss: {training_loss:.6f}, \\\n",
    "            Validation loss: {validation_loss:.6f}\")\n",
    "    print(f\"Epoch {epoch}, Training accuracy: {trainnig_accuracy:.6f}, \\\n",
    "            Validation accuracy: {validation_accuracy:.6f}\")\n",
    "    \n",
    "    if abs(validation_loss - min_loss) > eps:\n",
    "        min_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        print(\"Save the model --> model.pt\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=24, bias=True)\n",
       "  (fc4): Linear(in_features=24, out_features=4, bias=True)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload model\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0461,  0.4024, -1.0115,  0.2167],\n",
      "        [-0.6123,  0.5036,  0.2310,  0.6931],\n",
      "        [-0.2669,  2.1785,  0.1021, -0.2590]], requires_grad=True)\n",
      "tensor([[ 1.0461,  1.4024, -0.0115,  1.2167],\n",
      "        [ 0.3877,  1.5036,  1.2310,  1.6931],\n",
      "        [ 0.7331,  3.1785,  1.1021,  0.7410]], grad_fn=<AddBackward0>)\n",
      "tensor(1.1853, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "w = torch.randn(3, 4, requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "x = w * torch.tensor([1, 1, 1, 1]) + 1\n",
    "x.retain_grad()\n",
    "print(x)\n",
    "\n",
    "z = torch.mean(x)\n",
    "print(z)\n",
    "\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0833, 0.0833, 0.0833, 0.0833],\n",
       "        [0.0833, 0.0833, 0.0833, 0.0833],\n",
       "        [0.0833, 0.0833, 0.0833, 0.0833]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0833, 0.0833, 0.0833, 0.0833],\n",
       "        [0.0833, 0.0833, 0.0833, 0.0833],\n",
       "        [0.0833, 0.0833, 0.0833, 0.0833]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2 * a\n",
    "\n",
    "b.retain_grad()   # Since b is non-leaf and it's grad will be destroyed otherwise.\n",
    "\n",
    "c = b.mean()\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 80.6603,  -0.3962],\n",
      "        [223.6336,   2.8058],\n",
      "        [134.6960,  -0.4273],\n",
      "        [158.6102,   2.9853],\n",
      "        [106.7771,  -5.0731]])\n",
      "tensor([[ 26.1258, -42.2493, -99.2625,  -8.8227, 160.3694, -28.2195,  48.8896],\n",
      "        [  0.6235,   1.8275,   2.0791,   1.4693,   4.4247,   1.7089,  -1.6074]])\n",
      "tensor([[ 80.6382,  -0.3962],\n",
      "        [223.5714,   2.8058],\n",
      "        [134.6590,  -0.4273],\n",
      "        [158.5659,   2.9853],\n",
      "        [106.7489,  -5.0732]])\n",
      "tensor([[ 26.1188, -42.2330, -99.2254,  -8.8183, 160.3078, -28.2089,  48.8725],\n",
      "        [  0.6235,   1.8277,   2.0794,   1.4693,   4.4241,   1.7090,  -1.6075]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 2, 5, 2, 7\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        print(w1.grad)\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        print(w2.grad)\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25., 20., 15.])\n"
     ]
    }
   ],
   "source": [
    "class MyLayer(nn.Module):\n",
    "  def __init__(self, param):\n",
    "    super().__init__()\n",
    "    self.param = param \n",
    "  \n",
    "  def forward(self, x):\n",
    "    return x * self.param\n",
    "  \n",
    "myLayerObject = MyLayer(5)\n",
    "output = myLayerObject(torch.Tensor([5, 4, 3]) )    #calling forward inexplicitly \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True), Parameter containing:\n",
      "tensor([[-1.8922e-01,  3.8575e-02, -5.8615e-02,  6.8145e-02, -7.5199e-02,\n",
      "         -2.0701e-01,  4.1986e-02,  2.1418e-01, -1.8987e-01,  2.7720e-01],\n",
      "        [ 7.7828e-02,  3.1181e-01, -2.8985e-01, -3.0270e-01,  1.0728e-01,\n",
      "         -2.9501e-01, -2.5915e-01,  1.6080e-01,  1.5351e-01, -2.8815e-01],\n",
      "        [ 2.2755e-01, -6.1489e-02,  2.3477e-01, -1.8558e-01, -3.0069e-01,\n",
      "         -2.0963e-01, -4.9024e-02, -3.1582e-01, -2.1546e-02, -1.5782e-01],\n",
      "        [-7.3603e-02, -4.4972e-05,  1.7748e-02, -3.0933e-01,  1.6266e-01,\n",
      "         -1.9382e-01,  1.9272e-01,  2.8977e-01, -1.0546e-01,  3.5491e-02],\n",
      "        [ 1.7436e-01, -2.5054e-02, -2.6460e-01,  1.4205e-01,  1.5044e-01,\n",
      "         -1.7325e-01, -2.1292e-01, -5.5187e-02,  6.2001e-02, -1.5992e-01]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0076, -0.1367, -0.1232, -0.0433, -0.1205], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class net2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Linear(10,5) \n",
    "    self.tens = nn.Parameter(torch.ones(3,4))                       # This will show up in a parameter list \n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.linear(x)\n",
    "\n",
    "myNet = net2()\n",
    "print(list(myNet.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[ 0.0867, -0.1081,  0.0585],\n",
      "          [ 0.0793,  0.0411, -0.0754],\n",
      "          [ 0.1451, -0.1044, -0.1212]],\n",
      "\n",
      "         [[ 0.0483,  0.0630,  0.0113],\n",
      "          [ 0.0056,  0.0625,  0.1055],\n",
      "          [-0.0721,  0.0468, -0.0549]],\n",
      "\n",
      "         [[-0.0771, -0.0774, -0.1109],\n",
      "          [ 0.1258, -0.0505, -0.0419],\n",
      "          [-0.0036, -0.0971,  0.0966]],\n",
      "\n",
      "         [[-0.0464,  0.0792,  0.0433],\n",
      "          [-0.1480, -0.1371, -0.1453],\n",
      "          [ 0.1338,  0.1360,  0.0510]],\n",
      "\n",
      "         [[-0.0620,  0.0490, -0.1126],\n",
      "          [ 0.0242, -0.0204,  0.1021],\n",
      "          [ 0.0094, -0.0007, -0.0132]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1431, -0.1043,  0.0676],\n",
      "          [-0.1341,  0.0274, -0.1249],\n",
      "          [ 0.0724,  0.1334, -0.0715]],\n",
      "\n",
      "         [[ 0.1337, -0.1335,  0.0711],\n",
      "          [ 0.0254, -0.0341, -0.0206],\n",
      "          [ 0.0596, -0.0272, -0.1442]],\n",
      "\n",
      "         [[-0.0538,  0.1265, -0.1284],\n",
      "          [-0.0503,  0.0615,  0.1090],\n",
      "          [-0.1121, -0.1032,  0.0899]],\n",
      "\n",
      "         [[ 0.1014,  0.0970,  0.0856],\n",
      "          [-0.1271,  0.0491, -0.0054],\n",
      "          [ 0.0229, -0.0570,  0.1442]],\n",
      "\n",
      "         [[-0.0081, -0.1020,  0.0772],\n",
      "          [-0.0031, -0.0929, -0.0659],\n",
      "          [-0.0940,  0.0293,  0.0320]]],\n",
      "\n",
      "\n",
      "        [[[-0.0413,  0.0122, -0.0809],\n",
      "          [-0.1479, -0.0036,  0.1413],\n",
      "          [-0.1110,  0.0296, -0.0749]],\n",
      "\n",
      "         [[-0.0234,  0.0348, -0.0735],\n",
      "          [-0.1278, -0.0023, -0.0111],\n",
      "          [ 0.1079,  0.0595,  0.0857]],\n",
      "\n",
      "         [[-0.0937, -0.1397, -0.0901],\n",
      "          [-0.1198, -0.0052, -0.0443],\n",
      "          [-0.0137, -0.0421, -0.0246]],\n",
      "\n",
      "         [[ 0.0331,  0.1461, -0.0308],\n",
      "          [ 0.1224,  0.1339,  0.0713],\n",
      "          [-0.0394,  0.0396, -0.0977]],\n",
      "\n",
      "         [[-0.0127, -0.0697, -0.1142],\n",
      "          [-0.1262, -0.0538,  0.0372],\n",
      "          [-0.0031,  0.0051,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0426,  0.1002, -0.0357],\n",
      "          [ 0.0342, -0.0015, -0.0512],\n",
      "          [-0.1001, -0.0413, -0.1411]],\n",
      "\n",
      "         [[ 0.0177, -0.1185, -0.0518],\n",
      "          [-0.0582,  0.1278, -0.1215],\n",
      "          [-0.0356,  0.0534,  0.0421]],\n",
      "\n",
      "         [[ 0.1223,  0.0362,  0.0876],\n",
      "          [-0.0895,  0.0925,  0.0032],\n",
      "          [-0.0166, -0.0669,  0.0552]],\n",
      "\n",
      "         [[ 0.0772, -0.1091, -0.1153],\n",
      "          [ 0.0595, -0.1319, -0.1402],\n",
      "          [ 0.1078,  0.0474,  0.0115]],\n",
      "\n",
      "         [[-0.0009,  0.0619, -0.1065],\n",
      "          [ 0.1239, -0.0208,  0.0146],\n",
      "          [-0.1411,  0.0091, -0.0911]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0839, -0.1416,  0.1344],\n",
      "          [-0.0349, -0.0509, -0.0010],\n",
      "          [-0.1219,  0.0481,  0.1216]],\n",
      "\n",
      "         [[-0.0626, -0.0484, -0.1242],\n",
      "          [ 0.0363,  0.1098,  0.1214],\n",
      "          [ 0.0478,  0.0669, -0.1094]],\n",
      "\n",
      "         [[-0.0599,  0.1455,  0.0022],\n",
      "          [ 0.0606, -0.1104, -0.1230],\n",
      "          [ 0.1177, -0.0662, -0.0805]],\n",
      "\n",
      "         [[-0.0065, -0.0108,  0.0651],\n",
      "          [ 0.0930,  0.1323, -0.0441],\n",
      "          [-0.0387, -0.1169, -0.0757]],\n",
      "\n",
      "         [[-0.0279, -0.0763,  0.0037],\n",
      "          [-0.0320,  0.0857, -0.1302],\n",
      "          [-0.0030, -0.0564,  0.0100]]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0805, -0.1468, -0.0035,  0.0133,  0.1294], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3046, -0.0666,  0.3621,  0.4307,  0.0682],\n",
      "        [-0.2181,  0.2638, -0.3239, -0.3748,  0.0384]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2611,  0.1428], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "layer_list = [nn.Conv2d(5,5,3), nn.BatchNorm2d(5), nn.Linear(5,2)]\n",
    "\n",
    "class myNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList(layer_list)\n",
    "  \n",
    "  def forward(x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "\n",
    "net = myNet()\n",
    "\n",
    "print(list(net.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "tensor([[[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 3)\n",
    "print(x)\n",
    "\n",
    "y = torch.ones(2, 3, 3)\n",
    "print(y)\n",
    "\n",
    "z = x + y\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.SGD([{\"params\": Net.fc1.parameters(), 'lr' : 0.001, \"momentum\" : 0.99},\n",
    "                             {\"params\": Net.fc2.parameters()}], lr = 0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function randn:\n",
      "\n",
      "randn(...)\n",
      "    randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with random numbers from a normal distribution\n",
      "    with mean `0` and variance `1` (also called the standard normal\n",
      "    distribution).\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
      "    \n",
      "    The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.randn(4)\n",
      "        tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
      "        >>> torch.randn(2, 3)\n",
      "        tensor([[ 1.5954,  2.8929, -1.0923],\n",
      "                [ 1.1719, -0.4709, -0.1996]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.randn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
